---
title: Python 爬虫
jupyter: python3
---

爬虫就是**用程序模拟浏览器的行为**，发送请求给服务器，获取网页的内容，解析网页数据，提取信息并保存。

## 前置知识

### 爬虫学习的整体框架

1. 获取网站资源：`requests`，`httpx`，`playwright`
2. 解析（parse）和处理（process）：`beautifulsoup`，`selectolax`
3. 加载（load）和提取（extract）：`txt`，`json`，`MySQL`，`MongoDB`

以上工作管道（pipeline）也可以由 `scrapy`框架完成。

关于爬取的资源，最常见的是 HTML 代码，但有些网页可能返回的是 JSON 字符串（其中 API 接口大多采用这种形式）。网页中还包含各种二进制数据，如图片、视频和音频等。除了上述数据，网页中还有各种扩展名文件，如 CSS，JavaScript 和配置文件等。

### URL（Universal Resource Locator）

统一资源定位符。我们通过一个链接，即 URL，从互联网中找到某个资源。URL 的资本组成格式如下，其中中括号部分不是必要的：

`scheme://[username:password@]hostname[:port][/path][;parameters][?query][#fragment]`

- `scheme`：协议，又常常被称作 `protocol`。常用的有：`http`，`https`。
- `username`，`password`：在某些情况下 URL 需要提供用户名和密码才能访问，URL 大概是这样的：`https://admin:admin@ssr3.scrape.center`
- `hostname`：主机地址，它可以是域名 `www.baidu.com`或者 IP 地址 `8.8.8.8`。
- `port`：端口。这是**服务器**设定的服务端口。有些 URL 没有端口信息，这是因为使用了默认的端口。例如 http 协议的默认端口为 80，https 协议的默认端口为 443。
- `path`：路径，指的是网络资源在服务器中的指定地址。
- `parameters`：参数，用来指定访问某个资源时的附加信息，但由于现在 `parameters`用的很少，所以现在人们常常把 `query`的部分称作参数，并进行混用，但是严格来说，`parameters`是跟在 `;`符号后面的。
- `query`：查询，用来查询某类资源，如果有多个查询，则用 `&`分隔开。这是常见的，例如：`https://www.baidu.com/?wd=nba&ie=utf-8`。我们在 GET 设置请求参数时，实际上设置的是这个。
- `fragment`：片段，它是对资源描述的部分补充，可以理解为资源内部的书签。

### HTTP 的几个特征

`HTTP`：Hyper Text Transfer Protocol

1. HTTP 是**无连接的**：无连接的含义是限制每次连接只处理一个请求（request）。服务器（server）处理完客户（client）的请求，并收到客户的应答后，即断开连接采用这种方式可以节省传输时间。
2. HTTP 是**媒体独立的**：这意味着，只要客户端和服务器知道如何处理数据内容，任何类型的数据都可以通过 HTTP 发送。客户端以及服务器指定使用适合的 MIME-type 内容类型。
3. HTTP 是**无状态的**（stateless）：无状态是指协议对于事务处理没有记忆能力。缺少状态意味着如果后续处理需要前面的信息，则它必须重传。这样可能导致每次连接传送的数据量增大，另一方面，在服务器不需要先前信息时，它的应答就比较快。但实际上客户端和服务器的数据交换是“有状态的（stateful）”。客户端向服务器发送账号密码，然后服务器返回建立好的 cookie，该 cookie 被存储在用户的本地中（local storage）。之后客户端再发送请求时会带上该 cookie，服务器就能识别该 cookie 并允许操作，这样就实现了一种表面上的有记忆。

HTTP 和 HTTPS（Hyper Transfer Protocol over Secure Socket Layer）都是网络协议，HTTPS 可以理解为 HTTP 的安全版，现在更多的网站和 APP 正在朝着 HTTPS 的方向发展。

### 用户代理（user-agent）

用户代理实际上就是代表客户用以访问网络资源的软件（或者说“中介”）。它发起请求并引起服务器响应，同时解释（interpret）响应内容并将其呈现（render）给用户。不同的用户代理类型会有不同的呈现级别和类型。常见的用户代理就是浏览器。

### HTTP 动词（HTTP verbs）

也叫做请求方法（Request Method）。

1. GET：`retrieve a resource`
2. POST：`create a new resource`。POST 请求大多是在提交表单时发起，相比于 GET 请求中的参数包含在 URL 里面，数据可以在 URL 中看到，POST 请求的 UTL 不会包含这些数据，数据都是通过表单形式传输，会包含在请求体中。
3. PUT：`update resource`
4. DELETE：`delete a resource`

### HTTP 状态代码（HTTP status code）

客户端在发送请求后，服务器会给予客户端响应：是如何处理客户端发送的请求的。于是状态代码就会与每个响应一起发送给客户端。状态代码由三个数字组成，其中第一个数字是 general category，第二三个数字是 sub-category，以及 even more specific sub-category。通常状态代码后面还会有一个原因短语：

- `1xx`：信息性响应（informational responses），表示请求还在进行中。
- `2xx`：状态性响应（status responses），表示请求已经成功了。
  - `201`：创建成功（Created）
  - `204`：没有内容（No Content）
- `3xx`：重定向响应（redirection responses），表明请求的资源已经被移动。
  - `301`：永久移动（Move Permanently）
  - `302`：临时移动（Found）
- `4xx`：用户端错误响应（client error responses）
  - `404`：未找到（Not Found）
  - `403`：客户端禁止访问（Forbidden）
- `5xx`：服务器错误响应（server error responses）
  - `500`：内部错误（Internal Server Error）
  - `502`：网关错误（Bad Gateway）

在 python 的 requests 库中，可以直接查询这些状态码：

```py
import requests as r
print(r.codes.ok)			 # 200
print(r.codes.not_found)	  # 404
```

### HTTP 头部（HTTP Headers）

HTTP 协议是**可扩展的**，这是通过头部体现的。HTTP 头部包括请求头和响应头。

- 请求头

  我们可以通过使用头部来说明服务器要使用的附加信息，不过是否选择忽略头部还是选择以某种特殊方式解释头部，这都取决于服务器。

  1. `User-Agent`头部，服务器可以了解发出请求的客户端类型，如浏览器、移动设备、API 客户端等。
  2. `Accept`、`Accept-Language`、`Accept-Encoding`等头部使客户端能够告知服务器它能理解或偏好的**内容类型、语言和编码格式**。服务器据此提供最适合客户端的响应版本。
  3. `Cookie`头部，这是网站为了辨别用户，进行会话跟踪而存储在用户本地的数据。它的主要功能是为了维持当前访问会话。
  4. `Authorization`头部可以用于传递认证信息（如用户名和密码），而 `Referer`头部用于标识请求是从哪个页面发过来的，从服务器可以拿到这一信息并做相应的处理。
  5. `Content-Type`：也叫互联网媒体类型（Internet Media Type）或者 MIME 类型，在 HTTP 协议消息头中，它用来表示具体请求中的媒体类型信息。例如，`text/html`代表 HTML 格式，`image/gif`代表 GIF 图片，`application/json`代表 JSON 类型。

- 响应头

  响应头包含了服务器对请求的应答信息，主要的有：

  - `Content-Type`：文档类型，它指定了返回的数据是什么类型。
  - `Server`：包含了服务器的信息，例如名称，版本号等。
  - `Set-Cookie`：设置 Cookie。响应头中的 Set-Cookie 用于告诉浏览器将此内容放在 Cookie 中，下次请求时 Cookie 携带上。

头部是**键值对**的形式。

### Session 和 Cookie

在浏览网站的时候，我们经常会遇到需要登陆的情况。有些页面只有登录之后才可以访问。在登录之后可以连续访问很多次网站。但是有时候过一段时间就需要重新登录。还有一些网站，在打开浏览器时就自动登陆了，而且在很长时间内都不会失效。其实这里面都涉及了 Session 和 Cookie 的相关知识。

众所周知，网页可以分为静态网页和动态网页。静态网页加载速度快，编写简单，但功能单一，不支持根据用户输入或 URL 变化动态显示内容。相反，动态网页具有更多交互性和可变性，可以实现如用户登录和注册等复杂功能。

由于 HTTP 协议本身是无状态的，即每次请求和响应都是独立的。如果每次访问都要额外传递一些重复的请求才能获取后续响应，那未免太浪费资源了。所以为了在用户访问过程中维持状态（如登录信息），出现了两种技术：Session 和 Cookie。

**Session（会话）**指的是客户端和服务器之间进行的一连串的交互过程。这个过程可以由多个 HTTP 请求和响应组成。**Session 对象**是在服务器端实现的一个程序实体，用于存储特定用户**Session（会话）**所需的**属性及配置信息**。它是一个具体的数据结构，比如一个对象或字典，在服务器的内存中维护用户会话的状态。Cookie 是某些网站为了鉴别用户身份，进行 Session 跟踪而**存储在用户本地终端上**的数据。

当一个用户首次向应用程序发起请求而服务器上尚未为该用户建立 Session 时，服务器会自动创建一个新的 Session 对象，并生成一个唯一的 Session ID。**这个 Session ID 随后被存储在一个 Cookie 中**，并通过 HTTP 响应（带有 Set-Cookie 响应头）发送给用户的浏览器。用户的浏览器接收并保存这个 Cookie，从而在随后的请求中，Cookie 会被自动地发送回服务器。服务器通过 Cookie 中保存的 Session ID 定位到相应的 Session 对象，利用其中的信息来维护用户的状态和数据。Session 对象在一定时间内没有活动（即不活跃）或通过特定操作（如用户登出）被显式地终止时，会被服务器标记为过期并最终被删除。这种机制允许服务器在 HTTP 的无状态环境中维护跨多个请求的用户状态，从而提供连续的用户体验。

一个 Cookie 中可以有多个 cookies，我们可以打开 dev tool 进行查看，==一个 cookie 条目==会拥有以下内容：

- `Name`：Cookie 的名称。Cookie 一旦创建，名称便不可更改。
- `Value`：Cookie 的值。如果值为 Unicode 字符，则需要为字符编码。如果值为二进制数据，则需要使用 BASE64 编码。
- `Domain`：指定可以访问该 Cookie 的域名。例如设置 Domain 为 github.com，表示所有以 github.com 结尾的域名都可以访问该 Cookie。
- `Path`：Cookie 的使用路径。如果设置为 `/path/`,则只有路径为 `/path/`的页面才可以访问该 Cookie。如果设置为 `/`，则本域名下的所有页面都可以访问该 Cookie。
- `Max-Age/Expires`：Cookie 失效的时间。
- `Size`：Cookie 的大小。
- `HTTPonly`：Cookie 的 `httponly`属性。若此属性为 true，则只有在 HTTP Headers 中才会带有此 Cookie 的信息，而不能通过 `document.cookie`来访问此 Cookie。
- `Secure`：是否仅允许使用安全协议传输 Cookie。安全协议有 HTTPS 和 SSL 等，使用这些协议在网络上传输数据之前会先将数据加密。其默认值为 false。

有**会话 Cookie**和**持久 Cookie**的划分，会话 Cookie 就是把 Cookie 放在浏览器内存里，关闭浏览器之后，Cookie 即失效；持久 Cookie 则会把 Cookie 保存到客户端的硬盘中，下次还可以继续使用。用于长久保持用户的登录状态。但严格来说，也没有这么明确的划分，只是 `Max-Age/Expires`字段决定了 Cookie 失效的时间。一些 Cookie 的有效时间和 Session 有效期设置的比较长，所以形成了持久化登录的网站。

### 代理服务器（Proxy Server）

由于互联网的分层性质，实际上我们的请求和响应会在它们到达目的地之前经过多个中介。代理服务器就是这些中介，它在客户端和终端服务器之间**转发请求和响应**。该过程中 Web 服务器识别出来的真实 IP 就不再是客户端的 IP 了，成功实现了 IP 的伪装。一些常见的代理服务器作用：

1. 内容缓存，提高访问速度：caching proxy 缓存代理服务器，它的功能就是缓存经常请求的内容，当同样的内容被再次请求时，可以更快地提供服务。
2. 突破自身 IP 的访问限制，访问一些平时不能访问的站点。或者隐藏自身真实 IP。

代理的分类：

- 根据匿名程度：

  - 透明代理（Transparent Proxy）：透明代理会转发请求，但同时会改动数据包，告知目标服务器用户的真实 IP 地址。这种代理除了能用缓存技术提高浏览速度，用内容过滤提高安全性之外，并无其他显著作用。
  - 普通匿名代理（Anonymous Proxy）：普通匿名代理会对数据包做一些改动，不会透露用户的真实 IP 地址，但会告知目标服务器请求是通过代理发送的。服务器有可能会发现正在访问自己的是个代理服务器，并且有一定概率去追查客户端的真实 IP。
  - 高匿代理（Elite Proxy）：将数据包原封不动地转发，完全隐藏用户的真实 IP 地址，对目标服务器完全透明，看起来就像是直接访问。记录的 IP 则是代理服务器的 IP。

- 根据协议类型：

  - HTTP 代理 ：主要用于访问网页。一般有内容过滤和缓存功能。
  - FTP 代理：主要用于访问 FTP 服务器。一般有上传，下载以及缓存过程。
  - SSL/TSL 代理：主要用于访问加密网站，一般有 SSL 或 TSL 加密功能。
  - SOCKS 代理：只是单纯传递数据包，不关心具体协议和用法，所以速度快很多，一般有缓存功能。

常见的代理设置如下：

- 对于网上的免费代理，最好使用高度匿名代理，可以在使用前把所有代理都抓取下来筛选一下可用代理，也可以进一步维护一个代理池。
- 使用付费代理服务。
- ADSL 拨号，拨一次号换一次 IP，稳定性高。
- 蜂窝代理

### HTML，CSS，JavaScript

HTML、CSS 和 JavaScript 是构成网页的三大核心技术。我们爬虫就是为了获取网页的内容，解析网页的数据。

关于 JavaScript：许多现代网站采用 Ajax、前端模块化工具构建，整个页面使用 JavaScript 渲染，这使得原始 HTML 是一个空壳。因而你从浏览器看到的可能并不是你得到的。在这些特定情况下，可能需要分析源代码后台 Ajax 接口，也可使用 `Selenium`，`Splash`等库模拟 JavaScript 渲染。

### HTTP 的消息结构

我们可以看下主要的流程和概念：

- 请求
  - 地址（URL）和请求方式（HTTP 动词）
  - 请求头（request header）：用来描述请求和发送者的一些信息。
  - 请求体：一般承载的内容是 POST 请求中的表单数据，对于 GET 请求，请求体则为空。
- 响应
  - 响应状态码
  - 响应头
  - 响应体：这是最关键的部分，响应的正文数据都存在于响应体中，例如请求网页时。响应体可能时网页的 HTML 代码；请求一张图片时，响应体就是图片的二进制数据。我们做爬虫请求网页时，要解析的内容就是响应体。

### 正则表达式

- 字符

  | 表达式  | 描述                                                    |
  | ------ | ------------------------------------------------------ |
  | [...]  | 字符集，也叫字符组。匹配集合中所含的任一字符。              |
  | [^...] | 否定字符集。匹配任何不在集合中的字符。                     |
  | [a-z]  | 字符范围。匹配指定范围内的任意字符。                       |
  | .      | 匹配除换行符（`\n`）以外的任何单个字符。                   |
  | \      | 转义字符。                                               |
  | \w     | 匹配任何字母数字，包括下划线（等价于 `[A-Za-z0-9_]`）。    |
  | \W     | 匹配任何非字母数字下划线（等价于 `[^A-Za-z0-9_]`）。       |
  | \d     | 匹配任何数字。等价于 `[0-9]`                             |
  | \D     | 非数字。匹配任何非数字字符。                              |
  | \s     | 空白。匹配任何空白字符，包括空格、制表符、换行等。         |
  | \S     | 非空白。匹配任何非空白字符。                             |

- 分组与引用

  | 表达式         | 描述                                                                          |
  |------------------------------------------------------------------------------------------------  |
  | (expression)   | 分组。匹配括号里的整个表达式。                                                  |
  | (?:expression) | 非捕获分组。匹配括号里的整个字符串但不获取匹配结果，拿不到分组引用。                             |
  | \num           | 对前面所匹配分组的引用（回溯引用）。\num 用于引用之前已经匹配的，即表示引用第 num 个分组的内容。 |

- 锚点与边界

  | 表达式  | 描述                                                                       |
  |---------|--------------------------------------------------------------------------|
  | ^      | 匹配字符串或行开头。                                                        |
  | $      | 匹配字符串或行结尾。                                                        |
  | \b     | 匹配单词边界。例如，`Sheep\b`可以匹配 `CodeSheep`末尾的 `Sheep`，不能匹配 `CodeSheepCode`中的 `Sheep`。 |
  | \B     | 匹配非单词边界。例如，`Code\B`可以匹配 `HelloCodeSheep`中的 `Code`，不能匹配 `HelloCode`中的 `Code`。   |

- 数量表示

  | 表达式 |  描述                                                   |
  | ------ | ------------------------------------------------------ |
  | ?      | 匹配前面的表达式 0 个或 1 个。即表示可选项。非贪婪方式     |
  | `+`    | 匹配前面的表达式至少 1 个。相当于 `{1,}`               |
  | `\*`     | 匹配前面的表达式 0 个或多个。相当于 `{0,}`             |
  | \|     | 或运算符。并集，可以匹配符号前后的表达式。             |
  | {m}    | 匹配前面的表达式 m 个。                                |
  | {m,}   | 匹配前面的表达式最少 m 个。                            |
  | {m,n}  | 匹配前面的表达式最少 m 个，最多 n 个。贪婪方式         |

  通用匹配，贪婪与非贪婪匹配：

  ==贪婪匹配==是正则表达中另外一个值得注意的点，前面写过，`{m, n}`匹配个数最少 m 个，最多 n 个，即可以匹配 m 个数字也可以匹配 n 个数字，不过当有 n 个数字的时候，优先匹配的是 n 个数字，这是因为默认为贪婪模式，即尽可能的匹配更多字符，而要使用==非贪婪==模式，需要在表达式后面加上 `?`号，即 `{m,n}?`。此外，我们常常会使用 `.*`进行通用匹配（同时匹配多个任意字符），如果要实现非贪婪匹配，需要使用 `.*?`。

  ```py
  import re
  content='Hello 1234567 World_This is a Regex Demo'
  result=re.match('^He.*(\d+).*Demo$', content)
  print(result.group(1))
  # 此时由于贪婪匹配，分组引用后的只有数字7

  # 若使用非贪婪匹配
  result=re.match('^He.*?(\d).*?Demo$')
  ```

  在 python 中实现正则匹配，主要使用 `re`库：

  - 修饰符：

    最常用的修饰符有

    - `re.S`：使匹配内容包括换行符在内的所有字符。由于绝大部分 HTML 文本包含换行符，**所以要尽量加上该修饰符**，以免出现匹配不到的问题。
    - `re.I`：使匹配对大小写不敏感。

  - 转义匹配

    当在目标字符串中遇到用作正则匹配的特殊字符时，在此字符前面加转义字符 `\`转义以下即可。

  - `re.match()`：match 匹配方法需要向它传入要匹配的字符串以及正则表达式。它会尝试从字符串的起始位置开始匹配正则表达式，如果匹配，就返回匹配成功的结果，如果不匹配，就返回 None。如果正则表达式中使用了分组引用，那么可以用 `xx.group(a)`来进行提取。

    ```py
    import re
    content='Extra String Hello 1234567 World_This is a Regex Demo'
    result=re.search('He.*?(\d+).*Demo$', content)
    print(result)
    print(result.group(1))
    # 使用.group()来提取分组内容，括号内填写具体的分组序号
    # 此时result为None，分组会报错
    ```

  - `re.search()`：由于 match 方法只能从开头进行匹配，一旦开头不匹配，整个匹配就失效了。而 search 方法则可以在匹配时扫描整个字符串，然后返回第一个匹配成功的结果，换句话说，正则表达式可以是字符串的一部分。如果扫描完还没有找到符合规则的字符串，就返回 None。

    这也侧面说明我们实际在写 search 方法的正则表达式时，不需要用 `^`，`$`这些锚点边界字符。

  - `re.findall()`：search 方法同样返回与正则表达式相匹配的第一个字符串。如果想要获取与正则表达式相匹配的所有字符串需要借助 `findall`方法。返回的结果是**列表**类型，可以循环提取。
  - `re.sub()`：除了使用正则表达式提取信息，有时候还需要借助它来修改文本。sub 方法可以对指定的内容进行替换。
  - `re.compile()`：compile 方法可以将正则字符串翻译成正则表达式对象，以便在后面的匹配中复用。

  以下是一个例子：

  ```py
  html = '''<div id="songs-list">
  <h2 class="title">经典老歌</h2>
  <p class="introduction">
  经典老歌列表
  </p>
  <ul id="list" class="list-group">
  <li data-view="2">一路上有你</li>
  <li data-view="7">
  <a href="/2.mp3" singer="任贤齐">沧海一声笑</a>
  </li>
  <li data-view="4" class="active">
  <a href="/3.mp3" singer="齐秦">往事随风</a>
  </li>
  <li data-view="6"><a href="/4.mp3" singer="beyond">光辉岁月</a></li>
  <li data-view="5"><a href="/5.mp3" singer="陈慧琳">记事本</a></li>
  <li data-view="5">
  <a href="/6.mp3" singer="邓丽君">但愿人长久</a>
  </li>
  </ul>
  </div>'''
  ```

  ```py
  # 我们需要提取所有歌名，这时候<a>元素中的内容对我来说可能价值不大，可以先使用sub方法简化，然后再使用findall方法。
  import re
  new_html=re.sub('<a.*?>|</a>', '', html)
  pattern=re.compile('<li.*?>(.*?)</li>')
  results=re.findall(pattern, new_html, re.S)
  # 列表形式进行循环提取（注意有的元素存在换行符）
  for result in results:
      print(result.strip())
  ```

### XPath 路径语言

在 `XPath`中，有七种类型的**节点**：元素、属性、文本、命名空间、处理指令、注释以及文档节点。元素是一种节点类型，通常指具有开始标签和结束标签的部分。

```xml
<bookstore> (元素节点，同时是根元素)

<author>J K. Rowling</author> (元素节点)

J K. Rowling (文本节点)

lang="en" (属性节点)
```

XML 文档是被作为节点树来对待的：

```xml
<?xml version="1.0" encoding="UTF-8"?>

<bookstore>

<book>
  <title lang="eng">Harry Potter</title>
  <price>29.99</price>
</book>

<book>
  <title lang="eng">Learning XML</title>
  <price>39.95</price>
</book>

</bookstore>
```

- 选取节点

  在下面的表格中，我们列出了一些路径表达式，以及这些表达式的结果：

  | 表达式   | 描述                                             |
  |----------|-------------------------------------------------|
  | nodename | 选取当前节点的所有名为 `nodename`的**子元素。      |
  | /        | 当出现在表达式的开始时（如 `/node`），它表示从根元素开始的绝对路径；当它出现在路径的中间或后面时（如 `node1/node2`或 `//node1/node2`），表示相对于前面部分指定的元素的路径。 |
  | //       | 在**整个文档中**搜索匹配的节点，不考虑它们在文档中的具体位置。        |
  | .        | 选取当前节点。                                    |
  | ..       | 选取当前节点的父节点。                             |
  | @        | 选取属性。                                        |


- 选取未知节点

  XPath 还支持通配符用来选取未知的 XML 元素。

  | 通配符  | 描述                     |
  | -----   | ----------------------- |
  | `\*`     | 匹配任何**元素节点**。   |
  | `@\*`    | 匹配任何**属性节点**。   |
  | node() | 匹配**任何类型的节点**。 |

  以下是一些示例：
 
  | 路径表达式      | 结果            |
  |----------------|-----------------|
  | `/bookstore/\*`   | 选取 bookstore 元素的所有子元素。 |
  | `//\*`          | 选取文档中的所有元素。              |
  | bookstore       | 从当前节点选取所有名为 `bookstore`的子元素。
  | bookstore/book  | 从当前节点开始，选择所有名为 `bookstore`的子元素中的所有名为 `book`的子元素。|
  | //book          | 选取文档中所有名为 `book`的元素，不考虑它们的具体位置或层级。          |
  | bookstore//book | 从当前节点开始，选择所有名为 `bookstore`的子元素，并从这些 `bookstore`元素中选择所有后代中的 `book`元素，不管这些 `book`元素位于 `bookstore`下的哪个层级。 |
  | `//@lang `        | 选取名为 lang 的所有**属性**，获得的是该属性的值。            |

- 谓语

  谓语用来查找**某个特定的节点**或者**包含某个指定的值的节点**。

  谓语被嵌在方括号中。一些常见的例子有：

  | 路径表达式                          | 结果                  |
  |------------------------------------|-----------------------|
  | /bookstore/book[1]                  | 选取属于 bookstore 子元素的第一个 book 元素。           |
  | /bookstore/book[last()]             | 选取属于 bookstore 子元素的最后一个 book 元素。         |
  | /bookstore/book[last()-1]           | 选取属于 bookstore 子元素的倒数第二个 book 元素。       |
  | /bookstore/book[position()<3]       | 选取最前面的两个属于 bookstore 元素的子元素的 book 元素。|
  | //title[@lang]                      | 选取所有拥有名为 lang 的属性的 title 元素。             |
  | //title[@*]                         | 选取所有**带有属性**的 title 元素。                    |
  | //title[@lang='eng']                | 选取所有 title 元素，且这些元素拥有值为 eng 的 lang 属性。|
  | /bookstore/book[price>35.00]        | 选取 bookstore 元素的所有 book 元素，且其中的 price 元素的值须大于35.00。|
  | /bookstore/book[price>35.00]//title | 选取 bookstore 元素中的 book 元素的**所有 title 元素**，且其中 book 的 price 元素的值须大于 35.00。 |

- 轴

  轴可定义相对于当前节点的节点集。

  | 轴名称             | 结果                                                     |
  | :----------------- | :------------------------------------------------------- |
  | **ancestor**       | 选取当前节点的所有先辈（父、祖父等）。                   |
  | ancestor-or-self   | 选取当前节点的所有先辈（父、祖父等）以及当前节点本身。   |
  | **attribute**      | 选取当前节点的所有属性。                                 |
  | **child**          | 选取当前节点的所有子元素。                               |
  | **descendant**     | 选取当前节点的所有后代元素（子、孙等）。                 |
  | descendant-or-self | 选取当前节点的所有后代元素（子、孙等）以及当前节点本身。 |
  | **following**      | 选取文档中当前节点的**结束标签之后**的**所有**节点。     |
  | following-sibling  | 选取当前节点之后的所有**兄弟节点**（同级）               |
  | namespace          | 选取当前节点的所有命名空间节点。                         |
  | **parent**         | 选取当前节点的父节点。                                   |
  | **preceding**      | 选取文档中当前节点的**开始标签之前**的所有节点。         |
  | preceding-sibling  | 选取当前节点之前的所有同级节点。                         |
  | self               | 选取当前节点。                                           |

- 运算符

  | 运算符 | 描述           | 实例                      | 返回值         |
  |--------|---------------|---------------------------|---------------|
  | **\|** | 计算两个节点集 | //book\| //cd             | 返回所有拥有 book 和 cd 元素的节点集|
  | `+`    | 加法        | 6 + 4                     | 10            |
  | `-`    | 减法          | 6 - 4                   |   2           |
  | `\*`   | 乘法          | 6 `\*` 4                |   24          |
  | div    | 除法          | 8 div 4                | 2             |
  | **=**  | 等于          | price=9.80          | 如果 price 是 9.80，则返回 true。如果 price 是 9.90，则返回false。 | 
  | **!=** | 不等于         | price!=9.80        | 如果 price 是 9.90，则返回 true。如果 price 是 9.80，则返回 false。 |
  | <      | 小于           | price<9.80         | 如果 price 是 9.00，则返回 true。如果 price 是 9.90，则返回 false。 |
  | <=     | 小于或等于     | price<=9.80         | 如果 price 是 9.00，则返回 true。如果 price 是 9.90，则返回 false。 |
  | `>`    | 大于           | price>9.80          | 如果 price 是 9.90，则返回 true。如果 price 是 9.80，则返回 false。 |
  | `>=`   | 大于或等于     | price>=9.80         | 如果 price 是 9.90，则返回 true。如果 price 是 9.70，则返回 false。 |
  | or     | 或       | price=9.80 or price=9.70  | 如果 price 是 9.80，则返回 true。如果 price 是 9.50，则返回 false。 |
  | and    | 与       | price>9.00 and price<9.90 | 如果 price 是 9.80，则返回 true。如果 price 是 8.50，则返回 false。 |
  | mod    | 计算除法的余数 | 5 mod 2               |  1               |

---

## 发送请求获取资源

### urllib

选择 urllib 其实是不如选择 request 库的。但是它还是有一些好用的函数：

```pyhon
from urllib.requests import urlopen
```

### requests

`requests`库能够使得 HTTP 请求更加简单快捷，相比标准库中的 `urllib`，`requests` 提供了更简洁的 API。可以用很少的代码行完成复杂的网络请求。

```py
import requests as r
```

#### 指定获取方法

1. GET：

   我们在爬虫中最常使用的动词是 `get`。`requests`库使得请求用途更加明确：

   ```py
   url='https://quotes.toscrape.com/' 	# specify the url
   resp=r.get(url=url)			# get the response object
   ```

   当发送请求后，服务器会返回一个 response 对象，格式形如 `<Response [xxx]>`，括号内为状态代码。

   可以进一步对响应对象提取信息，例如：

   ```py
   resp.status_code		# get the status code
   resp.headers	    		# get the response headers
   resp.request	    		# get the request that leads the response
   resp.request.url		# get the url
   resp.request.headers	 	# get the request headers
   resp.cookies		   	# get the cookie
   resp.text			# get the web resources
   resp.json()
   resp.content
   ```

   关于 `resp.text`，`resp.json()`以及 `resp.content`，它们是处理 HTTP 响应的不同方法，用于获取响应数据。三者的区别在于返回的类型不同：

   - `resp.text`：返回的是原始的响应文本，没有进行任何额外的处理或解析。这种形式返回的是字符串（`str`）类型。

     ```py
     type(resp.text)		# str
     ```

     `resp.text`会根据 HTTP**响应头**中的字符编码（如 `Content-Type`字段）来解码响应内容。不同的服务器响应类型决定了 `resp.text`返回的格式，这也就解释了为什么有些网页使用该函数后提取的内容是 JSON 格式的字符串，而有些直接是 HTML 格式的字符串：

     ```py
     json_resp=r.get('https://www.httpbin.org/get')
     # check the response headers
     json_resp.headers['Content-Type']
     # 'text/html; charset=utf-8'
     json_resp.text

     html_resp=r.get('https://quotes.toscrape.com/')
     html_resp.headers['Content-Type']
     # 'application/json'
     html_resp.text
     ```

   - `resp.json()`： 尝试将响应内容解析为 JSON 格式，并将其转换为 Python 中的字典。

     ```py
     type(resp.json())	# dir
     dir(resp.json())	# make it easier to read
     ```

     这是一种方便的方法，当知道响应内容是 JSON 格式时使用。它自动处理 JSON 的解析，省去了手动处理的麻烦。但如果响应内容不是有效的 JSON 格式，调用 `resp.json()` 会抛出一个异常，发生了解析错误（通常是 `json.JSONDecodeError`）。

   - `resp.content`：由于图片、音频、视频这些文件本质上都是由二进制码组成的，由于特定的保存格式和对应的解析方式，我们才可以看到这些形形色色的多媒体。如果仍使用 `resp.text`来抓取，会在解析成字符串的时候发生乱码，所以我们会使用 `resp.content`函数，返回 `bytes`类型：

     ```py
     import requests as r
     resp=r.get('https://scrape.center/favicon.ico')
     class(resp.content)
     ```

2. POST

   POST 请求是另一个常用的请求方法，即 `requests.post()`，根据目的的不同，会有不同的参数选择：

   - 模拟提交数据（`data`），待上传的数据以字典形式传入函数：

     ```py
     data={
       'name':'zhizhifan',
       'age':'23'
     }
     resp=r.post('https://www.httpbin.org/post', data=data)
     print(resp.text)
     ```

   - 提交文件（`files`），文件仍然需要以字典形式传入，值为文件的地址：

     ```py
     import requests as r
     resp=r.get('https://scrape.center/favicon.ico')

     with open('images/favicon.ico', 'wb') as f:
       f.write(resp.content)


     files={
       'icon':'images/favicon.ico'	# the address
     }

     resp=r.post('https://www.httpbin.org/post', files=files)
     print(resp.text)
     ```

#### 添加额外信息（请求查询参数）

请求参数是 URL 的一部分（作为查询字符串），一般跟随在 `?`后面，我们可以通过 `params`参数添加额外信息：

```py
base_url="https://api.sunrisesunset.io/json"
params={
  "lat":43.6532,
  "lng":-79.3832,
  "timezone":"EST",
  "date":"today"
}
resp=r.get(url=base_url, params=params)
```

#### 添加请求头

之前介绍过，头部是带有附加信息的键值对形式，我们可以通过代码向请求中添加头部（作为参数加入请求函数中），

1. 例如改变 `User-Agent`请求头内容：

   ```py
   headers={
       'User-Agent':'Mozilla/5.0';
   }
   # a directionary
   resp=r.get(url=url,headers=headers)
   ```

   在未进行指定的时候，请求头的 `User-Agent`是类似 `python-requests/2.29.0`的形式。

2. 设置 Cookie：

   我们可以先登录一个网站，这里以我的 `github`账号登录网页，然后获取请求头中的 Cookie：

   ```py
   cookie='_octo=GH1.1.80203512.1679826844; preferred_color_mode=light; tz=Asia/Shanghai; _device_id=fa442c86034220bea313f1b3f47b2501; tz=Asia/Shanghai; color_mode={"color_mode":"auto","light_theme":{"name":"light","color_mode":"light"},"dark_theme":{"name":"dark","color_mode":"dark"}}; saved_user_sessions=128157054:EhJHm-cWJ8_nuGFl5AOtXz3FWRdiOxdf-2a083T-6njXZMJI; user_session=EhJHm-cWJ8_nuGFl5AOtXz3FWRdiOxdf-2a083T-6njXZMJI; __Host-user_session_same_site=EhJHm-cWJ8_nuGFl5AOtXz3FWRdiOxdf-2a083T-6njXZMJI; logged_in=yes; dotcom_user=zhizhifan; has_recent_activity=1; _gh_sess=aSb3tQrFnTYoYaAp1oyA+Tnw0e/PeqLrv7yKPg3ubl/nakN2fVgfgv2WK1NosygiXklRmLZ7mr0/FastfZmj69czm8PvXv/vQ4X97qF/AEAA+xxipRL2N/dRY0+BHQRZngGlBHPGTRGn+a2JXh5hzufTxv3JafRBbYS9lx+w72XIQ9kGNnljz9JgfGYQFKv1w6DGhGphvbxNr7p7/hJPJglrAeQ+SD5iaLrBnJbVty1bGEG3tExJdYJz4zlbI6rUc2tf9TMBmyrPyhyvzNg1Z7zhRrDqGNVDY6e37/XiT3l7p4pRcw/Lq5HyQY9XPu8IJeHgDGtZ5kMqPgFS655B725mQlHYvVexu0Mq0mEXGYQOS6AVxzjUMUJaFefpOW/QyLQqyE0WtJcGF+xs/eb/D1BYYznlyL5fcZcqljhMD4itA1oRBPZbFG94rmWzfK/e3jJ91yFmU1puMUWzB6k8r8o/Jh25Rh40Y5Ik/2nahOg6JgeRmoxVWkwmfPrI4JHKypvtZ1NBGBNwrw5amm/mhg==--R/T0A1MkxzKG5s8X--berbFS6TAzkL3qimcnJq8w=='
   ```

   可以看到确实是键值对的形式，名称和对应的值，然后通过请求头进行设置请求：

   ```py
   headers={
     'Cookie':cookie
   }

   resp=r.get('https://github.com/', headers=headers)
   print(resp.text)
   ```

   这就完成了模拟登陆的操作。

#### 获取和设置 Cookie

requests 库获取 Cookie 的方法也很简单，先获得 `response`对象，然后使用 `xx.cookies`。注意这里获得的是从响应头中获得的 `Set-Cookie`信息，而不是请求头中的 `Cookie`。

```py
import requests as r
resp=r.get('https://www.baidu.com')
print(resp.cookies)
```

得到的是 `RequestsCoolieJar`类型。例如：`<RequestsCookieJar[<Cookie BDORZ=27315 for .baidu.com/>]>`。

然后我们可以调用 `items`方法将 Cookie 转化为==由元组组成的列表==，遍历输出：

```py
for key, value in resp.cookies.items():
    print(key + '=' + value)
```

关于设置 cookie，则在上面关于设置请求头部分介绍过。这里介绍另外一个方法，当对一个响应对象应用 `resp.cookies`获取响应 Cookie 时，我们获得了 `RequestsCookieJar`对象。所以我们也可以构造这个对象，然后将它传入 `requests.get()`函数的 `cookie`参数中：

```py
# 对原有cookie字符串进行分割处理，获得列表。循环处理提取key，value构造RequestsCookieJar对象。
import requests as r
jar=r.cookies.RequestsCookieJar()
for cookie_item in cookie.split(';'):
    key, value=cookie_item.split('=', 1)
    jar.set(key, value)
# split(separator, maxsplit) maxsplit指定了分割操作的最大次数为1，即遇到第一个分隔符（等号）时分割字符串，然后停止。
# 这将产生两部分：第一部分是等号之前的内容，第二部分是等号之后的所有内容。

resp=r.get('https://github.com/', cookie=jar)
print(resp.text)
```

#### Session 维持

之前提到，HTTP 协议是无状态的，为了克服这种独立性带来的限制，我们通常会使用 Session 和 Cookie 技术。设想这样一个场景：首先使用 POST 方法登录某个网站，然后想要通过 GET 方法获取登录后的个人信息。如果没有持续的会话管理，这就像在两个独立的浏览器中操作，彼此没有联系，因此无法获取到个人信息。每次请求都单独设置 Cookie 是一种解决方案，但这既繁琐又容易出错。

在爬虫编程中，为了解决这个问题，我们可以使用 Session 对象。正如前面所介绍的，Session 对象在服务器端代表一个用户会话，并存储了会话所需的状态信息。在爬虫中，使用编程语言提供的 Session 对象，我们可以模拟一个持续的会话。这样，当爬虫在两次请求间使用相同的 Session 对象时，就可以保持状态（如登录凭证），而不必担心每次都处理 Cookie。**Session 对象会自动管理发送和接收 Cookie**，使得连续的请求看起来就像是来自同一个用户。

使用 requests 库的 `requests.session()`创建 Session 对象，这可以模拟在一个浏览器中打开同一站点的不同页面。

```py
import requests as r
s=r.Session()
resp=s.get()
```

#### SSL 验证

现在很多网站要求使用 HTTPS 协议，但是有些网站可能并没有设置好 HTTPS 证书，或者网站的 HTTPS 证书可能并不被 CA 机构认可，这时这些网站就可能出现 SSL 证书错误的提示，例如当我们获取 `'https://ssr2.scrape.center/'`这个网页时：

```py
import requests as r
resp=r.get('https://ssr2.scrape.center/')
# 这时候会直接抛出SSLError错误，这是因为我们请求的URL证书是无效的
```

如果要强制爬取该网页的内容，可以使用 `verify`参数：

```py
resp=r.get('https://ssr2.scrape.center/', verify=False)
print(resp.status_code)
print(resp.text)
```

但这时候还会出现警告，为了忽略警告，可以采取以下措施：

```py
from requests.packages import urllib3
urllib3.disable_warnings()
resp=r.get('https://ssr2.scrape.center/', verify=False)
print(resp.status_code)
```

当然我们还可以指定一个本地证书用作客户端证书，这可以是单个文件（包含密钥和证书）或一个包含两个文件路径的元组，通过 `cert`参数传入。

#### 超时设置

如果本机网络状况不好或者服务器网络响应太慢甚至无响应时，我们可能会等待特别久的时间才能接受到响应，甚至到最后因为接受不到响应而报错。为了防止服务器不能及时响应，应该设置一个超市时间，如果超过这个时间还没有得到响应，就报错。使用 `timeout`参数。

```py
import requests as r
resp=r.get('https://www.httpbin.org/get', timeout=1)
# 设置超时时间为1秒。如果一秒内没有响应，就抛出异常
print(r.status_code)
```

事实上，请求分为两个阶段，连接（connect）和读取（read），上述设置方式是用作连接和读取的 `timeout`总和，如果想要分开单独设置，可以传入一个元组。

```py
resp=r.get('https://www.httpbin.org/get', timeout=(3, 50))
```

当然，也可以选择永久等待------直接将 `timeout`设置为 `None`，这和不设置 `timeout`参数是一样的，因为默认值就是 `None`。

#### 身份认证

在访问启用了基本身份认证的网站时，首先会弹出一个认证窗口，这时我们就要先模拟登陆进行身份认证，然后再进行后续操作，同时身份认证常常会结合 Session 维持。

requests 库提供了简单的写法，如果 `auth`参数直接传入一个元组，则默认使用 `HTTPBasicAuth`这个类来验证：

```py
import requests as r
resp=r.get('https://ssr3.scrape.center/')
print(resp.status_code)
# 这个网站要身份认证，此时状态码返回401
```

```py
resp=r.get('https://ssr3.scrape.center/', auth=('admin', 'admin'))
print(resp.status_code)
# 身份认证成功，状态码返回200
```

此外，requests 库还提供了其他认证方式，如 `OAuth`认证，使用 `OAuth1`认证的示例方法如下：

```py
import requests as r
from requests_oauthlib import OAuth1
url='https://api.twitter.com/1.1/account/verify_credentials.json'
auth=OAuth1('YOUR_APP_KEY', 'YOUR_APP_SECRET', 'USER_OAUTH_TOKEN', 'USER_OAUTH_TOKEN_SECRET')
resp=r.get(url=url, auth=auth)
```

#### 代理设置

某些网站在测试的时候请求几次，都能正常获取内容，但是一旦开始大规模爬取，面对大规模且频繁的请求时，这些网站就可能弹出验证码，或者跳转到登录认证页面，更甚者可能会直接封禁客户端的 IP，导致在一定时间段无法访问，为了防止这种情况发生，需要设置代理来解决这个问题，使用 `proxies`参数，向其传递一个代理字典来设置代理。这个字典指定了不同协议下的代理服务器地址，包括 IP 地址和端口：

```py
import requests as r
proxies={
  'http':'http://10.10.10.10:1080',
  'https':'http://10.10.10.10:1080'
}
# 设置不同协议下的代理服务器地址
resp=r.get('https://www.httpbin.org/get', proxies=proxies, timeout=10)
print(resp.status_code)
```

若代理需要身份验证，可以对服务器的地址进行改写，加上额外的用户名和密码两个信息，大概是：`http://user:password@host:port1` 的形式。

除了基本的 HTTP 代理外，requests 库还支持 `SOCKS`协议的代理。

```py
proxies={
  'http':'socks5://user:password@host:port',
  'https':'socks5://user:password@host:port'
}
# socks5协议进行代理
resp=r.get('https://www.http.org/get', proxies=proxies)
print(resp.status_code)
```

## 解析和处理网页数据

### 正则表达式匹配

前置知识有简短介绍。这里不再赘述。正则表达式的写法过于繁琐，且一旦有一个地方写错了，可能就会导致匹配失败。

### XPath 路径匹配

#### lxml

正则表达式虽然功能强大，但确实存在编写复杂且容易出错的问题，特别是在处理结构化的数据，如网页文档时。相比之下，使用 `XPath`和 CSS 选择器进行节点定位更为高效和直观。

`XPath`，即 XML 路径语言，是一种用于定位 XML 文档中节点的语言，也广泛用于 HTML 文档搜索。通过定义具体的路径表达式，`XPath`能够高效地定位到文档中的特定部分，极大地简化了数据提取过程。

在 Python 中，我们常用 `lxml`库来配合 `XPath`进行 HTML 文档的解析。`lxml`是一个功能强大且易于使用的库，它不仅支持 `XPath`，还支持 CSS 选择器，使得从 HTML 文档中提取数据变得更加简单。

我们将使用 `lxml`库的 `etree`模块。

```py
from lxml import etree
```

##### 构造 XPath 解析对象

当我们获得的是 HTML 文本时，可以接着调用 HTML 类进行初始化，这样就成功构造了一个 XPath 对象：

```py
import requests as r
from lxml import etree
resp=r.get('https://www.baidu.com')
html=etree.HTML(resp.text)
```

`etree`模块可以自动修正 HTML 文本，之后调用 `tostring`方法即可输出修正后的 HTML 代码，但是结果是 `bytes`类型，于是利用 `decode`方法将其转换成 `str`类型：

```py
text=etree.tostring(html)
print(text.decode('uft-8'))
```

当我们拥有的是 HTML 或 XML 文件时，构造 XPath 对象需要另外一个函数：`etree.parse()`：

```py
html=etree.parse('test.html', etree.HTMLParser())
text=etree.tostring(html)
print(text.decode('utf-8'))
```

##### 使用 XPath 进行解析

当构造好 XPath 解析对象后，我们对其进行解析 `xx.xpath('路径语法')`，获得列表对象。

大部分 XPath 路径语法已经在前置知识部分介绍过，这里稍作扩展：

1. 提取文字和属性值

   这是声明的 HTML 文本：

   ```py
   text = '''
   <div>
       <ul>
            <li class="item-0"><a href="link1.html">first item</a></li>
            <li class="item-1"><a href="link2.html">second item</a></li>
            <li class="item-inactive"><a href="link3.html">third item</a></li>
            <li class="item-1"><a href="link4.html">fourth item</a></li>
            <li class="item-0"><a href="link5.html">fifth item</a>
        </ul>
    </div>
    '''
   ```

   提取文字使用 `text()`

   ```py
   from lxml import etree
   html=etree.HTML(text)
   result=html.xpath('//li[@clss="item-inactive"]/a/text()')
   print(result)
   ```

   提取属性：

   ```py
   result=html.xpath('//li/@class')
   for attr in result:
       print(attr)
   ```

   当某一元素存在同一属性多值的情况，要使用 `contains`方法：

   ```py
   text='''
   <li class="li li-first"><a href="link.html">first item</a></li>
   '''
   ```

   ```py
   result=html.xpath('//li[contains(@class, "li")]/a/text()')
   print(result)
   # 这时若使用 html.xpath('//li[@class="li"]') 则只会获得空列表
   ```

   若某一元素存在多个属性的情况，要学会使用 XPath 运算符，如 `|`。

2. 善用 XPath 轴

   注意 XPath 轴的使用方法，要在轴名称的后面紧跟 `::`符号：

   ```py
   result=html.xpath('//li[1]/following-sibling::*/attribute::*')
   print(result)
   ```

### Beatiful Soup

前面已经介绍过正则表达式和 XPath 路径，现在还有更为简单的方法：`Beautiful Soup`。

`Beautiful Soup`是 Python 的一个 HTML 或 XML**解析库**，它提供一些**简单的，Python 式的函数**处理导航，搜索，修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以无需很多代码就可以写出一个完整的应用程序。

但实际上 `Beautiful Soup`在解析时是依赖解析器的，我们一般会使用**LXML 解析器**，因为它又解析 HTML 和 XML 的功能，而且速度快，容错能力强。

#### 构造 Beautiful Soup 对象

对于一个 HTML 文本字符串，我们可以先将其传给 `BeautifulSoup()`函数，然后再指定解析器类型。对于那些非标准的 HTML 字符串，它在此过程中会被自动更正为正确的格式。

```py
html = '''
<html><head><title>The Dormouse's story</title></head>
<body>
<p class="title" name="dromouse"><b>The Dormouse's story</b></p>
<p class="story">Once upon a time there were three little sisters; and their names were
<a href="http://example.com/elsie" class="sister" id="link1"><!-- Elsie --></a>,
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
and they lived at the bottom of a well.</p>
<p class="story">...</p>
'''

from bs4 import BeautifulSoup
soup=BeautifulSoup(html, 'lxml')
# 指定解析对象和解析器类型
```

然后使用 `prettify`方法把要解析的字符串以标准的缩进格式输出：

```py
print(soup)
print(soup.prettify())
```

#### 选择节点

- 单个**元素节点**选择

  ```py
  html = """
  <html>
      <head>
          <title>The Dormouse's story</title>
      </head>
      <body>
          <p class="story">
              Once upon a time there were three little sisters; and their names were
              <a href="http://example.com/elsie" class="sister" id="link1">
                  <span>Elsie</span>
              </a>
              <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a>
              and
              <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>
              and they lived at the bottom of a well.
          </p>
          <p class="story">...</p>
  """

  from bs4 import BeautifulSoup
  soup=BeautifulSoup(html, 'lxml')

  print(soup.title)
  print(soup.p)
  print(type(soup.title))

  直接调用元素节点的名称即可选择节点。当单个元素结构层次非常清晰时，可以选用这种方式来解析。关于该方法，需要注意两点：

  1. 当有多个同一元素时，这种选择方式只会匹配到第一个匹配的元素节点，后面的都会被忽略。
  2. 该类方法获得的对象都是 `bs4.element.Tag`，这是 `Beautiful Soup`中一个重要的数据结构，**经过选择器选择的结果都是这种Tag类型**，如 `title`元素的选择结果是这样：`<title>The Dormouse's story</title>`，它是 `bs4.element.Tag`对象。
  ```

- 嵌套选择

  嵌套选择有点类似于后代选择器：

  ```py
  print(soup.head.title)
  # 选择head元素下的title元素，仍然是bs4.element.Tag对象
  ```

- 关联选择

  这里需要注意的是，几乎所有关联选择默认选取的都是**节点**，也就是说不仅仅选择元素节点，文本、换行符（当换行符后面紧跟文本时，两者被当作同一个节点）会被视作节点被选中。

  - 获取**子节点和子孙节点**

    `children`属性用来选取直接子节点，返回的结果是生成器类型，可以转换成列表类型，或者直接 for 循环输出

    ```py
    for i, child in enumerate(soup.p.children):
      print(i, child)

    print(list(soup.p.children))
    ```

    `descendants`属性用来选取所有子孙节点。它包括了后代节点本身以及它们的文本内容。返回结果仍然是生成器类型。

    ```py
    for i, descendant in enumerate(soup.p.descendants):
        print(i, descendant)
    ```

  - 获取**父节点和祖先节点**

    `parent`属性用来获取直接父节点，`parents`属性用来获取祖先节点。输出父节点或者祖先节点的全部内部内容。返回属性仍然是生成器类型。

    ```py
    for i, parent in enumerate(soup.p.parents):
        print(i, parent)
    ```

  - 获取**兄弟节点**

    总共可以用 4 种属性，`next-sibling`，`next-siblings`，`previous-sibling`，`previous-siblings`。

- 调用方法选择器

  之前介绍的方法都不是很好的选择，因为格式化输出总会带来一些换行符号，导致不必要的误解和后续操作。所以我更推荐 `find_all`和 `find`方法。

  `find_all`，顾名思义就是查询所有符合条件的元素，可以给它传入一些属性或文本来得到符合条件的元素，它的 API 如下：

  ```py
  find_all(name, attrs, recursive, text, **kwargs)
  ```

  其中 `attrs`参数需要传入字典：

  ```py
  html='''
  <div class="panel">
      <div class="panel-heading">
          <h4>Hello</h4>
      </div>
      <div class="panel-body">
          <ul class="list" id="list-1">
              <li class="element">Foo</li>
              <li class="element">Bar</li>
              <li class="element">Jay</li>
          </ul>
          <ul class="list list-small" id="list-2">
              <li class="element">Foo</li>
              <li class="element">Bar</li>
          </ul>
      </div>
  </div>
  '''

  from bs4 import BeautifulSoup
  soup=BeautifulSoup(html, 'lxml')
  print(soup.find_all(name='ul', attrs={'id':'list-1'}))
  # 最后所有符合查找条件的元素都被放置在列表当中，可以循环提取
  print（type(soup.find_all(name='ul', attrs={'id':'list-1'}))
  # 结果仍然是Tag类型
  ```

  `attrs`参数中一些常见的属性可以单独写出来，比如 `class`和 `id`属性，其中由于 class 是 python 中的关键字，所以需要改成 `class_`的形式：

  ```py
  print(soup.find_all(class_='list'))
  print(soup.find_all(id='list-1'))
  ```

  对于 `text`参数用来查找符合匹配的**文本节点**，也就是说最后返回的是**文本内容的列表**，而不是 Tag 类型。参数可以传入字符串，或者正则表达式：

  ```py
  html='''
  <div class="panel">
      <div class="panel-body">
          <a id="first-anchor">Hello, this is a link</a>
          <a>Hello, this is a link, too</a>
      </div>
  </div>
  '''
  from bs4 import BeautifulSoup
  soup=BeautifulSoup(html, 'lxml')
  pattern=re.compile('link')
  print(soup.find_all(text=pattern))
  ```

  `find`方法与 `find_all`方法类似，只不过只能匹配第一个满足条件的节点。此外还有一些其他选择方法，这与前面是对应的：`find_parent`，`find_parents`，`find_next_sibling`，`find_next_siblings`，`find_previous_sibling`，`find_previous_siblings`，这些方法一般会结合嵌套选择，例如：

  ```py
  print(
      soup.find(class_='list').find_parents()
  )
  ```

- CSS 选择器

  Web 开发中经常会用到 CSS 选择器，`BeautifulSoup`库也提供了 `select`方法来实现此功能，匹配的对象仍然是 Tag 类型：

  ```py
  print(soup.select('.panel-body'))
  print(soup.select('.panel-body a'))
  print(soup.select('#first-anchor'))
  ```

#### 提取信息

经过选择器选择的结果都是 Tag 类型，Tag 具有一些属性，调用该属性可以获得相应信息。

- 获取名称

  调用 `name`属性获得节点名称

- 获取属性

  调用 `attrs`属性获得节点的属性

  ```py
  print(soup.select(class_='panel-body').attrs)
  # 获取全部属性，返回的类型是字典
  print(soup.select(class_='panel-body').attrs['class'])
  # 获取指定的属性内容
  print(soup.select(class_='panel-body')['class'])
  # 同上，另一种写法
  ```

  值得注意的是，某些属性的值是唯一的，于是返回结果就是单个字符串。但对于 `class`属性，一个元素可能同时拥有多个 class，所以返回的就是列表（尽管它们目前可能只指定了一个 class）。

- 获取内容

  调用 `string`属性，或者使用 `get_text()`。

  ```py
  for element in soup.select('#list-1 .element'):
    print(element.string, element.get_text())
  ```

### Pyquery

Beautiful Soup 对于 CSS 选择器的功能没有那么强大，如果对于 CSS 选择器更熟悉的话，`pyquery`是个更好的选择。

## 数据的存储

## Ajax 数据爬取

